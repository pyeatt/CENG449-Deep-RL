\documentclass[12pt]{exam}

\usepackage{tikz}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture,node distance=0pt]\node(#1){};}
\tikzstyle{line} = [draw, very thick, color=black!80, -latex']


\newif\ifanswers
\answersfalse
%\answerstrue

\usepackage{amsmath}
\usepackage{amssymb}

\newsavebox{\tmpbox}
\usepackage{times}
\usepackage{mathptmx}
\usepackage{multicol}
\usepackage{graphics}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{tabularx}
%\usepackage{hyperref}
%\usepackage{arm}
%\usepackage{lstautogobble}
%\lstloadlanguages{C}
%\usepackage{eforms} % <-- the driver is pdftex or xetex

\topmargin 0in
\headheight 0in
\headsep 0in
\textwidth 6.5in
\textheight 9in
\evensidemargin 0in
\oddsidemargin 0in

\usepackage{color}
\usepackage{xcolor}
\newcommand{\lstcolor}{
\lstset{
  commentstyle=\color{green},
  keywordstyle=[1]\color{blue},
  keywordstyle=[2]\color{cyan},
  keywordstyle=[3]\color{orange},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{purple}
}}

\newcommand{\lstnocolor}{
\lstset{
  commentstyle=\color{black},
  keywordstyle=[1]\color{black},
  keywordstyle=[2]\color{black},
  keywordstyle=[3]\color{black},
  numberstyle=\tiny\color{black},
  stringstyle=\color{black}
}}

\newlength{\mybasewidth}
\settowidth{\mybasewidth}{\ttfamily\small m}
\newlength{\mynormalbasewidth}
\settowidth{\mynormalbasewidth}{\ttfamily m}
\newlength{\footnotebasewidth}
\settowidth{\footnotebasewidth}{\ttfamily\footnotesize m}

\lstset{language=C,
  backgroundcolor=\color{white},  % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}  
  basicstyle=\ttfamily\small,
  columns=fixed,
  mathescape=true,
  basewidth=\mybasewidth,
  breakatwhitespace=true,        % sets if automatic breaks should only happen at whitespace
  breaklines=false,               % sets automatic line breaking
%  captionpos=b,                   % sets the caption-position to bottom
  commentstyle=\color{green},   % comment style
%  escapeinside={\{*}{*\}},         % if you want to add LaTeX within your code
  extendedchars=false,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  escapechar=`,
  frame=single,                   % adds a frame around the code
  keywordstyle=[1]\color{blue},      % keyword style
  keywordstyle=[2]\color{cyan},      % keyword style
  keywordstyle=[3]\color{orange},
  numbers=left,                   % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                  % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,               % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,         % underline spaces within strings only
  showtabs=false,                 % show tabs within strings adding particular underscores
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{purple},     % string literal style
  tabsize=8                     % sets default tabsize to 2 spaces
%  title=\lstname                  % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\usepackage{pdfbase}
\makeatletter
\ExplSyntaxOn
  \newcommand{\textField}[2]{
    % register current font in /AcroForm <<...>> PDF dictionary
    \pbs_add_form_font:
    % get current text colour
    \extractcolorspec{.}\@tempb
    \expandafter\convertcolorspec\@tempb{rgb}\@tempb
    \edef\@tempa{\expandafter\@rgbcomp\@tempb\@nil}
    % insert Text Field
    \raisebox{0.4\depth}{\makebox[#2][l]{
      \pbs_pdfannot:nnnn{#2}{\ht\strutbox}{\dp\strutbox}{
        /Subtype/Widget/FT/Tx/T (#1)
        % set font, size, current colour
        /DA (\pbs_last_form_font:\space\f@size\space Tf~\@tempa\space rg)
        /MK<</BC [0~0~0]/BG [0.9~0.9~0.9]>>
      }\strut
    }}
    % register Text Field in /AcroForm
    \pbs_appendtofields:n{\pbs_pdflastann:}
  }
\ExplSyntaxOff
\def\@rgbcomp#1,#2,#3\@nil{#1 #2 #3} %helper
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% set of characters of the current font to be embedded
\usepackage{luatex85}
\newcommand{\embedChars}[1]{\pdfincludechars\font{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{CSC 449/549 Advanced Topics in Artificial Intelligence}
\date{Deep Reinforcement Learning\\Final Exam\\Fall, \the\year}

\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
%\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}

\begin{document}
\maketitle


\newlength{\mytabcolsep}
\setlength{\mytabcolsep}{0.75pt}
% set zerowidth to the width of '0' in the current font
\newlength{\zerowidth}
\settowidth{\zerowidth}{0}
\newlength{\normaltabcolsep}
\setlength{\normaltabcolsep}{\tabcolsep}


 %---------------------------------------------------------------------
 
\begin{question}{10}
Monte Carlo methods for learning value functions require episodic tasks. Why, specifically? How is it that $n$-step TD methods avoid this limitation and can work with continuing tasks?
  \begin{minipage}[t][2.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}

\end{question}

\begin{question}{20}
Your Monte-Carlo algorithm generates the following episode using
policy $\pi$ when interacting with its environment. This is the first
episode that has been generated.
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    Timestep & Reward & State & Action\\
    \hline
    \hline
0 & & $s_1$ & $a_1$ \\
    \hline
1 & 13 & $s_1$ & $a_2$ \\
    \hline
2 & 7 & $s_1$ & $a_1$ \\
    \hline
3 & 13 & $s_1$ & $a_2$ \\
    \hline
4 & 14 & $s_2$ &  \\
    \hline
  \end{tabular}
\end{center}
Assume the discount factor, $\gamma$, is 1, and $s_2$ is a terminal state.
\begin{subquestion}
  What are the estimates of: $q_\pi(s_1, a_1)$ and $q_\pi(s_1, a_2)$ if using first-visit?
  \begin{minipage}[t][2in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion}
  What are the estimates of: $q_\pi(s_1, a_1)$ and $q_\pi(s_1, a_2)$ if using every-visit?
  \begin{minipage}[t][2in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}

\end{question}

%-----------------------------------------------------------------------

\begin{question}{4}
  True or False?
  \vspace{\baselineskip}
  \begin{subquestion}
\rule{1cm}{0.15mm} Q-learning can learn the optimal Q-function $Q^*$ without ever executing the optimal policy.
  \end{subquestion}
  \vspace{\baselineskip}
  \begin{subquestion}
\rule{1cm}{0.15mm} If an MDP has a transition model $T$ that assigns non-zero probability for all triples $T (s, a, s')$
then Q-learning will fail.
\end{subquestion}
\end{question}

%-----------------------------------------------------------------------
%-----------------------------------------------------------------------

\begin{question}{16}
  What is the formal definition of a Partially Observable Markov Decision Process (POMDP), and why is it so much more difficult to find an optimal policy for a POMDP compared to a Completey Obesrvable Markov Decision process?
  \begin{minipage}[t][5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}

\end{question}

%-----------------------------------------------------------------------
\begin{question}{50}
A rat is involved in an experiment. It experiences one episode. At the
first step it hears a bell. At the second step it sees a light. At the
third step it both hears a bell and sees a light. It then receives
some food, worth $+1$ reward, and the episode terminates on the fourth
step. All other rewards were zero. The experiment is undiscounted.
  \vspace{\baselineskip}
\begin{subquestion} (7 pts)
Represent the ratâ€™s state $s$ by a vector of two binary features, $bell(s) \in \{0, 1\}$ and
$light(s) \in \{0, 1\}$. Write down the sequence of feature vectors corresponding to this
episode.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (7 pts)
Approximate the state-value function by a linear combination of these
features with two parameters: $b \cdot bell(s) + l \cdot light(s)$. If
$b = 2$ and $l = -2$ then write down the sequence of approximate
values corresponding to this episode.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (4 pts)
 Define the $\lambda$-return $v^\lambda_t$.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (7 pts)
Write down the sequence of $\lambda$-returns $v^\lambda_t$
 corresponding to this episode, for $\lambda = 0.5$
 and $b = 2$, $l = -2$.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\end{question}
\begin{continuequestion}
\begin{subquestion} (7 pts)
Using the forward-view TD($\lambda$) algorithm and your linear
function approximator, what are the sequence of updates to weight $b$?
What is the total update to weight $b$?  Use $\lambda = 0.5$, $\gamma = 1$, $\alpha = 0.5$ and start with $b = 2$, $l = -2$.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (4 pts)
Define the TD($\lambda$) accumulating eligibility trace $\mathbf{e_t}$
when using linear value function approximation.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (7 pts)
Write down the sequence of eligibility traces $\mathbf{e_t}$
corresponding to the bell, using $\lambda = 0.5$ and $\gamma = 1$,
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\begin{subquestion} (7 pts)
Using the backward-view TD($\lambda$) algorithm and your linear
function approximator, what are the sequence of updates to weight $b$?
(Use offline updates, i.e. do not actually change your weights, just
accumulate your updates). What is the total update to weight $b$? Use
$\lambda = 0.5$, $\gamma = 1$, $\alpha = 0.5$ and start with $b = 2$,
$l = -2$.
  \begin{minipage}[t][1.5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{subquestion}
\end{continuequestion}


%%  \pagebreak[4]

%% {\setlength{\parindent}{2em}

%%   % http://www.cs.umd.edu/~djacobs/CMSC828DeepLearning/Midterm.pdf

%%   Suppose we want to approximate the value (or policy) function with a
%%   neural network that has a loop in it (see Figure~\ref{fig:nn}).  It
%%   is really just multiple layers of linear regression (with a twist)
%%   with a feedback loop in it.  So when it is given an input, it
%%   follows a recurrent computation. We can index these computations by
%%   time. The input is a $d$-dimensional vector, $x$, with $x_0$ always
%%   being a constant 1.  The timeframe of the neural network is much
%%   faster than the timeframe of the RL algorithm, so the input $x$ can
%%   be considered to be constant over time. We are looking only at the
%%   behavior of the function approximaor within one RL timestep.

%%   Also, the output of each neuron is passed through a Rectified Linear Output (ReLU) transfer funtion. The ReLU function is defined as:
%%   $$
%%   \textrm{ReLu}(x)=
%%   \begin{cases}
%%     x & x > 0, \\
%%     0 & \textrm{otherwise.}
%%   \end{cases}
%%   $$

  
%%     At $t = 1$ we can assume that the output from node $a_2$ is zero, and compute the output $a_1(t)$ from node $a_i$ as
%%     $$a_i(1) = \textrm{ReLU}\left(\sum_{i=0}^d x_i w^1_i\right).$$
%%     Likewise, we can compute the outputs from nodes $a_2$ and $a_3$ as:
    
%%     $$a_2(1) = \textrm{ReLU}\left(x_0w^2_0 + a_1w^2_1\right),$$
%%     and
%%     $$a_3(1) = \textrm{ReLU}\left(x_0w^3_0 + a_2w^3_1\right),$$
%%     Since we know that $x_0$ is a constant $1$ we can also express this as 
%%     \begin{eqnarray}
%%       a_i(1) & =& \textrm{ReLU}\left(w^1_0 + \sum_{i=1}^d x_i w^1_i\right), \label{eq:bugger}\\
%% a_2(1) &=& \textrm{ReLU}\left(w^2_0 + a_1w^2_1\right),\textrm{and}\\
%% a_3(1) & = & \textrm{ReLU}\left(w^3_0 + a_2w^3_1\right).
%%     \end{eqnarray}

%%     However, at time $t>1$ we must also consider the feedback loop. For $t>1$, Equation~\ref{eq:bugger} becomes
    
%%     $$a_i(t>1) = \textrm{ReLU}\left(w^4 a_2 + \sum_{i=0}^d x_i w^1_i\right).$$

%% We believe that, eventually, the output $a_3$ will converge, but now
%% we have to run our function approximator until $a_3(t) \approx
%% a_3(t-1)$.  Suppose that we have to run for $T$ timesteps to achieve convergence.
%% Suppose our TD target for this iteration of the TD algorithm
%% is $y$, and our loss function is
%% $$
%% % L(x,y)=\frac{1}{2n}\sum_{i=1}^n \left(y - a_3(x^i,T)\right)^2
%% L(x,y)=\frac{1}{2} \left(y - a_3(x,T)\right)^2
%% $$
%% This is just mean squared error.

%% %% L(x, y) = 1
%% %% 2n
%% %% nâˆ‘
%% %% i=1
%% %% (yi âˆ’ a3(xi, T ))2
%% %% This is just the standard regression loss. yi denotes the iâ€™th label, xi denotes a
%% %% vector containing the iâ€™th input, and a3(xi, T ) denotes the network output after
%% %% T iterations, with an input of xi.
%% %% â€¢ If we want to train the network using gradient descent, what is the gradient
%% %% if we have T = 2?
%% %% 2
%% %% â€¢ Suppose we use the network by using T = âˆž, and running the network
%% %% until it converges. What is the gradient of the loss in this case?


  
%% %%   \begin{subquestion}
%% %%   \end{subquestion}
%% %%   \begin{minipage}[t][1in]{\linewidth}
%% %%   \end{minipage}
%% %%   \begin{subquestion}
%% %%   \end{subquestion}
%% %%   \begin{minipage}[t][1in]{\linewidth}
%% %%   \end{minipage}
%% }

%% \begin{question}{40}

%% \end{question}
%%  \begin{figure}
%%   {
%%     \centerline{\input{nn.pdf_t}}
%%   }
%%   \caption{A recurrent neural network with ReLU activation function.}
%%   \label{fig:nn}
%%   \end{figure}

%-----------------------------------------------------------------------

\end{document}
