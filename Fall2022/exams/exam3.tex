\documentclass[12pt]{exam}

\usepackage{tikz}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture,node distance=0pt]\node(#1){};}
\tikzstyle{line} = [draw, very thick, color=black!80, -latex']


\newif\ifanswers
\answersfalse
%\answerstrue

\usepackage{amsmath}
\usepackage{amssymb}

\newsavebox{\tmpbox}
\usepackage{times}
\usepackage{mathptmx}
\usepackage{multicol}
\usepackage{graphics}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{tabularx}
%\usepackage{hyperref}
%\usepackage{arm}
%\usepackage{lstautogobble}
%\lstloadlanguages{C}
%\usepackage{eforms} % <-- the driver is pdftex or xetex

\topmargin 0in
\headheight 0in
\headsep 0in
\textwidth 6.5in
\textheight 9in
\evensidemargin 0in
\oddsidemargin 0in

\usepackage{color}
\usepackage{xcolor}
\newcommand{\lstcolor}{
\lstset{
  commentstyle=\color{green},
  keywordstyle=[1]\color{blue},
  keywordstyle=[2]\color{cyan},
  keywordstyle=[3]\color{orange},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{purple}
}}

\newcommand{\lstnocolor}{
\lstset{
  commentstyle=\color{black},
  keywordstyle=[1]\color{black},
  keywordstyle=[2]\color{black},
  keywordstyle=[3]\color{black},
  numberstyle=\tiny\color{black},
  stringstyle=\color{black}
}}

\newlength{\mybasewidth}
\settowidth{\mybasewidth}{\ttfamily\small m}
\newlength{\mynormalbasewidth}
\settowidth{\mynormalbasewidth}{\ttfamily m}
\newlength{\footnotebasewidth}
\settowidth{\footnotebasewidth}{\ttfamily\footnotesize m}

\lstset{language=C,
  backgroundcolor=\color{white},  % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}  
  basicstyle=\ttfamily\small,
  columns=fixed,
  mathescape=true,
  basewidth=\mybasewidth,
  breakatwhitespace=true,        % sets if automatic breaks should only happen at whitespace
  breaklines=false,               % sets automatic line breaking
%  captionpos=b,                   % sets the caption-position to bottom
  commentstyle=\color{green},   % comment style
%  escapeinside={\{*}{*\}},         % if you want to add LaTeX within your code
  extendedchars=false,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  escapechar=`,
  frame=single,                   % adds a frame around the code
  keywordstyle=[1]\color{blue},      % keyword style
  keywordstyle=[2]\color{cyan},      % keyword style
  keywordstyle=[3]\color{orange},
  numbers=left,                   % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                  % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,               % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,         % underline spaces within strings only
  showtabs=false,                 % show tabs within strings adding particular underscores
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{purple},     % string literal style
  tabsize=8                     % sets default tabsize to 2 spaces
%  title=\lstname                  % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\usepackage{pdfbase}
\makeatletter
\ExplSyntaxOn
  \newcommand{\textField}[2]{
    % register current font in /AcroForm <<...>> PDF dictionary
    \pbs_add_form_font:
    % get current text colour
    \extractcolorspec{.}\@tempb
    \expandafter\convertcolorspec\@tempb{rgb}\@tempb
    \edef\@tempa{\expandafter\@rgbcomp\@tempb\@nil}
    % insert Text Field
    \raisebox{0.4\depth}{\makebox[#2][l]{
      \pbs_pdfannot:nnnn{#2}{\ht\strutbox}{\dp\strutbox}{
        /Subtype/Widget/FT/Tx/T (#1)
        % set font, size, current colour
        /DA (\pbs_last_form_font:\space\f@size\space Tf~\@tempa\space rg)
        /MK<</BC [0~0~0]/BG [0.9~0.9~0.9]>>
      }\strut
    }}
    % register Text Field in /AcroForm
    \pbs_appendtofields:n{\pbs_pdflastann:}
  }
\ExplSyntaxOff
\def\@rgbcomp#1,#2,#3\@nil{#1 #2 #3} %helper
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% set of characters of the current font to be embedded
\usepackage{luatex85}
\newcommand{\embedChars}[1]{\pdfincludechars\font{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{CSC 449 Advanced Topics in Artificial Intelligence}
\date{Deep Reinforcement Learning\\Exam 2\\Fall, \the\year}

\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
%\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}

\begin{document}
\maketitle

Your solutions to these problems should be uploaded to D2L as a single
pdf file by the deadline. You may turn in the
solution up to two days late, with a penalty of 10\% per day, and you
should only upload one version of your solutions.

This exam is individual and open book. You may consult any reference
work. If you make specific use of a reference outside those on the
course web page in solving a problem, include a citation to that
reference.

%% So, for example, if you consult a general reference to understand the
%% meaning of VC-dimension, you don’t need to cite that.  But if you find
%% a reference that contains a specific result that you draw on in
%% solving the problem on VC-dimension, please cite that.

You may discuss the course material in general with other students,
but you must work on the solutions to the problems on your own.

%% So, again, you can discuss the definition of VC-dimension with other
%% students. You can even go over the VC-dimension of a different problem
%% with other students. But you should not work on the specific problem
%% in this midterm with other students.

It is difficult to write questions in
which every possibility is taken into account. As a result, there may sometimes
be ``trick'' answers that are simple and avoid addressing the
intended problem. Such trick answers will not receive credit. As an
example, suppose we said, use the chain rule to compute $\frac{\partial z}{\partial x}$ with $z =
\frac{7}{y}$ and $y = x^2$. A trick answer would be to say that the partial
deriviative is not well defined because $y$ might equal $0$. A correct
answer might note this, but would then give the correct partial
derivative when $y \ne 0$.

\newlength{\mytabcolsep}
\setlength{\mytabcolsep}{0.75pt}
% set zerowidth to the width of '0' in the current font
\newlength{\zerowidth}
\settowidth{\zerowidth}{0}
\newlength{\normaltabcolsep}
\setlength{\normaltabcolsep}{\tabcolsep}

\begin{question}{40}
  Consider the following pseudo-code for a faulty SARSA algorithm:
%  \begin{algorithm}
%    \caption{Faulty SARSA with $\epsilon\textrm{-greedy}$ policy}
    \begin{algorithmic}
      \Procedure{SARSA}{
        %%     $\left(
        %%     \begin{minipage}{3in}\raggedright
        %%     Number of episodes $N\in\mathbb{N}$\hfill\linebreak
        %%     discount factor $\lambda\in(0,1]$\hfill\linebreak
        %%   learning rate $\alpha_n=\frac{1}{\log(n+1)}$
        %%     \end{minipage}
        %%     \right)$
        number of episodes $N\in\mathbb{N}$\newline
        \phantom{\textbf{procedure} SARSA (}
        discount factor $\lambda\in(0,1]$\newline
      \phantom{\textbf{procedure} SARSA (}
      learning rate $\alpha_n=\frac{1}{\log(n+1)}$
      }
        %\Comment{This is a test}
        \State Initialize matrices $Q(s,a)$ and $n(s,a)$ to $0, \forall s,a$
        \For{episode $k \in 1,2,3,\ldots,n$}
        \State $t\gets 1$
        \State Initialize $s_1$ 
        \State Choose $a_1$ from a uniform distribution over the actions
        \While {Episode $k$ is not finished}
        \State Take action $a_t$: observe reward $r_t$ and next state $s_{t+1}$
        \State Choose $a_{t+1}$ from $s_{t+1}$ using $\mu_t$: an $\epsilon\textrm{-greedy}$ policy with respect to $Q$
        \If {The current state is terminal} \Comment\emph{Compute target value}
        \State $$y_t=0$$
        \Else
        \State $$y_t=r_t + \max_a Q(s_{t+1},a)$$
        \EndIf
        \State $n(s_t,a_t) \gets n(s_t,a_t) + 1$
        \State Update Q function:
        $$
        Q(s_{t+1},a_{t+1})\gets Q(s_t,a_t) - \alpha_{n(s_t,a_t)}\left(y_t - Q(s_t,a_t)\right)
        $$
        \State $t\gets t + 1$
        \EndWhile
        \EndFor
        \EndProcedure
    \end{algorithmic}
%  \end{algorithm}
  Find all of the mistakes in the algorithm. Explain why they are mistakes, and correct them.
  \begin{minipage}[t][5in]{\linewidth}
    \ifanswers
    Put answer here
    \fi
  \end{minipage}
\end{question}

 %---------------------------------------------------------------------
 
\begin{question}{60}
  Your friend found a variant of SARSA which is defined through a sequence of policies $\pi_t$ (where $t\ge 1$), and consists of just changing (in the previous algorithm {\bf after corrections}) the way the target is computed. The target becomes
  $$
  y_t = r_t + \lambda \sum_a \pi_t(a|s_{t+1})Q(S_{t+1},a),
  $$
  where $\pi_t(a|s)$  is the probability that $a$ is selected in state $s$ under policy $\pi_t.$

  \begin{subquestion}
    What sequence of policies $(\pi_t)$ % _{t\ge 1}$
    should you choose so that the corresponding variant of SARSA is on-policy?
    This variant is called Expected SARSA.
  \begin{minipage}[t][1in]{\linewidth}
  \end{minipage}
  \end{subquestion}
  
  \begin{subquestion}
    Consider an off-policy variant of SARSA corresponding to a stationary policy $\pi=\pi_t \forall t.$ Under this algorithm, do the Q values converge? If so, what are the limiting Q values?  Justify your answer.
  \begin{minipage}[t][1in]{\linewidth}
  \end{minipage}
  \end{subquestion}

\end{question}


%-----------------------------------------------------------------------


%%  \pagebreak[4]

%% {\setlength{\parindent}{2em}

%%   % http://www.cs.umd.edu/~djacobs/CMSC828DeepLearning/Midterm.pdf

%%   Suppose we want to approximate the value (or policy) function with a
%%   neural network that has a loop in it (see Figure~\ref{fig:nn}).  It
%%   is really just multiple layers of linear regression (with a twist)
%%   with a feedback loop in it.  So when it is given an input, it
%%   follows a recurrent computation. We can index these computations by
%%   time. The input is a $d$-dimensional vector, $x$, with $x_0$ always
%%   being a constant 1.  The timeframe of the neural network is much
%%   faster than the timeframe of the RL algorithm, so the input $x$ can
%%   be considered to be constant over time. We are looking only at the
%%   behavior of the function approximaor within one RL timestep.

%%   Also, the output of each neuron is passed through a Rectified Linear Output (ReLU) transfer funtion. The ReLU function is defined as:
%%   $$
%%   \textrm{ReLu}(x)=
%%   \begin{cases}
%%     x & x > 0, \\
%%     0 & \textrm{otherwise.}
%%   \end{cases}
%%   $$

  
%%     At $t = 1$ we can assume that the output from node $a_2$ is zero, and compute the output $a_1(t)$ from node $a_i$ as
%%     $$a_i(1) = \textrm{ReLU}\left(\sum_{i=0}^d x_i w^1_i\right).$$
%%     Likewise, we can compute the outputs from nodes $a_2$ and $a_3$ as:
    
%%     $$a_2(1) = \textrm{ReLU}\left(x_0w^2_0 + a_1w^2_1\right),$$
%%     and
%%     $$a_3(1) = \textrm{ReLU}\left(x_0w^3_0 + a_2w^3_1\right),$$
%%     Since we know that $x_0$ is a constant $1$ we can also express this as 
%%     \begin{eqnarray}
%%       a_i(1) & =& \textrm{ReLU}\left(w^1_0 + \sum_{i=1}^d x_i w^1_i\right), \label{eq:bugger}\\
%% a_2(1) &=& \textrm{ReLU}\left(w^2_0 + a_1w^2_1\right),\textrm{and}\\
%% a_3(1) & = & \textrm{ReLU}\left(w^3_0 + a_2w^3_1\right).
%%     \end{eqnarray}

%%     However, at time $t>1$ we must also consider the feedback loop. For $t>1$, Equation~\ref{eq:bugger} becomes
    
%%     $$a_i(t>1) = \textrm{ReLU}\left(w^4 a_2 + \sum_{i=0}^d x_i w^1_i\right).$$

%% We believe that, eventually, the output $a_3$ will converge, but now
%% we have to run our function approximator until $a_3(t) \approx
%% a_3(t-1)$.  Suppose that we have to run for $T$ timesteps to achieve convergence.
%% Suppose our TD target for this iteration of the TD algorithm
%% is $y$, and our loss function is
%% $$
%% % L(x,y)=\frac{1}{2n}\sum_{i=1}^n \left(y - a_3(x^i,T)\right)^2
%% L(x,y)=\frac{1}{2} \left(y - a_3(x,T)\right)^2
%% $$
%% This is just mean squared error.

%% %% L(x, y) = 1
%% %% 2n
%% %% n∑
%% %% i=1
%% %% (yi − a3(xi, T ))2
%% %% This is just the standard regression loss. yi denotes the i’th label, xi denotes a
%% %% vector containing the i’th input, and a3(xi, T ) denotes the network output after
%% %% T iterations, with an input of xi.
%% %% • If we want to train the network using gradient descent, what is the gradient
%% %% if we have T = 2?
%% %% 2
%% %% • Suppose we use the network by using T = ∞, and running the network
%% %% until it converges. What is the gradient of the loss in this case?


  
%% %%   \begin{subquestion}
%% %%   \end{subquestion}
%% %%   \begin{minipage}[t][1in]{\linewidth}
%% %%   \end{minipage}
%% %%   \begin{subquestion}
%% %%   \end{subquestion}
%% %%   \begin{minipage}[t][1in]{\linewidth}
%% %%   \end{minipage}
%% }

%% \begin{question}{40}

%% \end{question}
%%  \begin{figure}
%%   {
%%     \centerline{\input{nn.pdf_t}}
%%   }
%%   \caption{A recurrent neural network with ReLU activation function.}
%%   \label{fig:nn}
%%   \end{figure}

%-----------------------------------------------------------------------

\end{document}
